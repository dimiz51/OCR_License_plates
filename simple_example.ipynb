{"cells":[{"cell_type":"markdown","metadata":{"id":"A72FsHx-tvBA"},"source":["# Simple OCR example"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23470,"status":"ok","timestamp":1716124406772,"user":{"displayName":"Dimitris Z","userId":"18123542160357697530"},"user_tz":-120},"id":"OOvWrz87twOJ","outputId":"a6462f80-4cbb-461e-8f07-8b05b82f8fd7"},"outputs":[],"source":["# from google.colab import drive\n","\n","# # Mount the Google Drive at /content/drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10262,"status":"ok","timestamp":1716124419178,"user":{"displayName":"Dimitris Z","userId":"18123542160357697530"},"user_tz":-120},"id":"Tc1Yu2v7t3kp","outputId":"d0fe366d-6c8c-4531-b4c6-c7170372e307"},"outputs":[],"source":["# !unzip /content/drive/MyDrive/dataset_ocr.zip"]},{"cell_type":"markdown","metadata":{},"source":["# Create train/val splits from original dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3909,"status":"ok","timestamp":1716124430853,"user":{"displayName":"Dimitris Z","userId":"18123542160357697530"},"user_tz":-120},"id":"dSZ1WuontvBF"},"outputs":[],"source":["# Original dataset from: https://www.kaggle.com/datasets/nickyazdani/license-plate-text-recognition-dataset\n","\n","from typing import List\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n","import pandas as pd\n","import os\n","import shutil\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tensorflow import keras\n","\n","# Create these and move downloaded dataset files to DATASET_ORIGINAL_PATH\n","DATASET_ORIGINAL_PATH = 'dataset_original'\n","OCR_DATASET_PATH = 'dataset_ocr'\n","\n","\n","def create_ocr_dataset(csv_path):\n","    df = pd.read_csv(csv_path)[['images', 'labels']]\n","    ds_dict = df.to_dict()\n","\n","    # Split the dataset into train, validation, and test sets\n","    train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","    train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42)\n","\n","    # Create directories for train, val, and test images\n","    for split in ['train', 'val', 'test']:\n","        os.makedirs(os.path.join(OCR_DATASET_PATH, split, 'img'), exist_ok=True)\n","        os.makedirs(os.path.join(OCR_DATASET_PATH, split, 'annot'), exist_ok=True)\n","\n","    # Copy images and create annotation files for train, val, and test sets\n","    for split, df_split in zip(['train', 'val', 'test'], [train_df, val_df, test_df]):\n","        for _, row in df_split.iterrows():\n","            # Copy image\n","            img_src = os.path.join(DATASET_ORIGINAL_PATH, 'images', row['images'])\n","            img_dst = os.path.join(OCR_DATASET_PATH, split, 'img', row['images'])\n","            shutil.copy(img_src, img_dst)\n","\n","            # Write annotation string to a text file\n","            annot_dst = os.path.join(OCR_DATASET_PATH, split, 'annot', os.path.splitext(row['images'])[0] + '.txt')\n","            with open(annot_dst, 'w') as f:\n","                f.write(row['labels'])\n","\n","create_ocr_dataset(os.path.join(DATASET_ORIGINAL_PATH, \"lpr.csv\"))"]},{"cell_type":"markdown","metadata":{"id":"LTEeliCZtvBI"},"source":["# Create Tensorflow Datasets for training"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1920,"status":"ok","timestamp":1716124456081,"user":{"displayName":"Dimitris Z","userId":"18123542160357697530"},"user_tz":-120},"id":"FfJPiHHjtvBI"},"outputs":[],"source":["# Define your constants\n","OCR_DATASET_PATH = 'dataset_ocr'\n","IMAGE_HEIGHT = 224\n","IMAGE_WIDTH = 224\n","MAX_SEQ_LENGTH = 6\n","BATCH_SIZE = 64\n","\n","ocr_characters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\"\n","\n","# Mapping characters to integers\n","char_to_num = layers.StringLookup(vocabulary=list(ocr_characters), mask_token=None)\n","\n","# Mapping integers back to original characters\n","num_to_char = layers.StringLookup(\n","    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",")\n","\n","def encode_single_sample(img_path, label):\n","    img = tf.io.read_file(img_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, [IMAGE_HEIGHT, IMAGE_WIDTH])\n","\n","    # Convert to gray scale\n","    img = tf.image.rgb_to_grayscale(img)\n","\n","    # Convert to float32 and normalize to range [0, 1]\n","    img = tf.cast(img, tf.float32) / 255.0\n","\n","    # Transpose so the width of the image batch the time dimension\n","    img = tf.transpose(img, perm=[1, 0, 2])\n","\n","    # Map the characters in label to numbers\n","    label_string = tf.io.read_file(label)\n","    label = char_to_num(tf.strings.unicode_split(label_string, input_encoding=\"UTF-8\"))\n","\n","    return {\"image\": img, \"label\": label}\n","\n","def create_tf_dataset(data_dir):\n","    img_files = sorted(os.listdir(os.path.join(data_dir, 'img')))\n","    annot_files = sorted(os.listdir(os.path.join(data_dir, 'annot')))\n","    img_paths = [os.path.join(data_dir, 'img', filename) for filename in img_files]\n","    annot_paths = [os.path.join(data_dir, 'annot', filename) for filename in annot_files]\n","\n","    dataset = tf.data.Dataset.from_tensor_slices((img_paths, annot_paths))\n","    dataset = dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","    return dataset.batch(batch_size = BATCH_SIZE, drop_remainder=True).prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","train_dataset = create_tf_dataset(os.path.join(OCR_DATASET_PATH, \"train\")).shuffle(buffer_size = 512)\n","val_dataset = create_tf_dataset(os.path.join(OCR_DATASET_PATH, \"val\")).shuffle(buffer_size = 512)\n","test_dataset = create_tf_dataset(os.path.join(OCR_DATASET_PATH, \"test\")).shuffle(buffer_size = 512)\n"]},{"cell_type":"markdown","metadata":{"id":"i3WUwm2Rnepc"},"source":["# Visualize some samples"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":438},"executionInfo":{"elapsed":5849,"status":"ok","timestamp":1716124465974,"user":{"displayName":"Dimitris Z","userId":"18123542160357697530"},"user_tz":-120},"id":"nyi80g_RmVoY","outputId":"0d715793-d3ed-4134-822b-7268d81e2900"},"outputs":[],"source":["_, ax = plt.subplots(4, 4, figsize=(10, 5))\n","for batch in train_dataset.take(1):\n","    images = batch[\"image\"]\n","    labels = batch[\"label\"]\n","    for i in range(16):\n","        img = (images[i] * 255).numpy().astype(\"uint8\")\n","        label = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode(\"utf-8\")\n","        ax[i // 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")\n","        ax[i // 4, i % 4].set_title(label, fontsize=8)\n","        ax[i // 4, i % 4].axis(\"off\")\n","\n","    plt.suptitle(\"Dataset Images\", fontsize=16)\n","    plt.subplots_adjust(hspace=0.5)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1716124472932,"user":{"displayName":"Dimitris Z","userId":"18123542160357697530"},"user_tz":-120},"id":"yhrCWyjhoFgM"},"outputs":[],"source":["# CTC Layer and parts from: https://keras.io/examples/vision/captcha_ocr/#inference\n","\n","def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n","    label_length = tf.cast(tf.squeeze(label_length, axis=-1), dtype=tf.int32)\n","    input_length = tf.cast(tf.squeeze(input_length, axis=-1), dtype=tf.int32)\n","    sparse_labels = tf.cast(ctc_label_dense_to_sparse(y_true, label_length), dtype=tf.int32)\n","\n","    y_pred = tf.math.log(tf.transpose(y_pred, perm=[1, 0, 2]) + tf.keras.backend.epsilon())\n","\n","    return tf.expand_dims(\n","        tf.compat.v1.nn.ctc_loss(\n","            inputs=y_pred, labels=sparse_labels, sequence_length=input_length\n","        ),\n","        1,\n","    )\n","\n","def ctc_label_dense_to_sparse(labels, label_lengths):\n","    label_shape = tf.shape(labels)\n","    num_batches_tns = tf.stack([label_shape[0]])\n","    max_num_labels_tns = tf.stack([label_shape[1]])\n","\n","    def range_less_than(old_input, current_input):\n","        return tf.expand_dims(tf.range(tf.shape(old_input)[1]), 0) < tf.fill(\n","            max_num_labels_tns, current_input\n","        )\n","\n","    init = tf.cast(tf.fill([1, label_shape[1]], 0), dtype=tf.bool)\n","    dense_mask = tf.compat.v1.scan(\n","        range_less_than, label_lengths, initializer=init, parallel_iterations=1\n","    )\n","    dense_mask = dense_mask[:, 0, :]\n","\n","    label_array = tf.reshape(\n","        tf.tile(tf.range(0, label_shape[1]), num_batches_tns), label_shape\n","    )\n","    label_ind = tf.compat.v1.boolean_mask(label_array, dense_mask)\n","\n","    batch_array = tf.transpose(\n","        tf.reshape(\n","            tf.tile(tf.range(0, label_shape[0]), max_num_labels_tns),\n","            tf.reverse(label_shape, [0]),\n","        )\n","    )\n","    batch_ind = tf.compat.v1.boolean_mask(batch_array, dense_mask)\n","    indices = tf.transpose(\n","        tf.reshape(tf.concat([batch_ind, label_ind], axis=0), [2, -1])\n","    )\n","\n","    vals_sparse = tf.compat.v1.gather_nd(labels, indices)\n","\n","    return tf.SparseTensor(\n","        tf.cast(indices, dtype=tf.int64),\n","        vals_sparse,\n","        tf.cast(label_shape, dtype=tf.int64)\n","    )\n","\n","class CTCLayer(tf.keras.layers.Layer):\n","    def __init__(self, name=None):\n","        super().__init__(name=name)\n","        self.loss_fn = ctc_batch_cost\n","\n","    def call(self, y_true, y_pred):\n","        batch_len = tf.cast(tf.shape(y_true)[0], dtype=tf.int64)\n","        input_length = tf.cast(tf.shape(y_pred)[1], dtype=tf.int64)\n","        label_length = tf.cast(tf.shape(y_true)[1], dtype=tf.int64)\n","\n","        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=tf.int64)\n","        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=tf.int64)\n","\n","        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n","        self.add_loss(loss)\n","\n","        return y_pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2181,"status":"ok","timestamp":1716124478750,"user":{"displayName":"Dimitris Z","userId":"18123542160357697530"},"user_tz":-120},"id":"KxXVKUewoSUF","outputId":"8fcde22a-af43-4bc7-bbff-fd40bf2cc1d9"},"outputs":[],"source":["def build_model(img_width, img_height, char_to_num):\n","    inputs = tf.keras.Input(shape=(img_width, img_height, 1), name=\"image\", dtype=tf.float32)\n","    labels = tf.keras.Input(name=\"label\", shape=(None,), dtype=tf.float32)\n","\n","    # CNN Blocks\n","    x = tf.keras.layers.Conv2D(\n","        32, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\", name=\"Conv1\"\n","    )(inputs)\n","    x = tf.keras.layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n","\n","    x = tf.keras.layers.Conv2D(\n","        64, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\", name=\"Conv2\"\n","    )(x)\n","    x = tf.keras.layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n","\n","    # Reshape before passing to RNN\n","    new_shape = ((img_width // 4), (img_height // 4) * 64)\n","    x = tf.keras.layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n","    x = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n","    x = tf.keras.layers.Dropout(0.2)(x)\n","\n","    # RNNs\n","    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n","    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n","\n","    # Output layer for inference model\n","    x = tf.keras.layers.Dense(len(char_to_num.get_vocabulary()) + 1, activation=\"softmax\", name=\"dense2\")(x)\n","\n","    # Add CTC layer (only for training model)\n","    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n","\n","    model = tf.keras.models.Model(inputs=[inputs, labels], outputs=output, name=\"simple_ocr\")\n","    model.compile(optimizer=tf.keras.optimizers.Adam())\n","\n","    return model\n","\n","model = build_model(IMAGE_WIDTH, IMAGE_HEIGHT, char_to_num)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1164438,"status":"ok","timestamp":1716125645053,"user":{"displayName":"Dimitris Z","userId":"18123542160357697530"},"user_tz":-120},"id":"263Rin9ypfTS","outputId":"81f9aa10-9c69-42f0-d16f-dce27a703413"},"outputs":[],"source":["epochs = 100\n","early_stopping_patience = 10\n","early_stopping = keras.callbacks.EarlyStopping(\n","    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",")\n","\n","# Train the model\n","history = model.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=epochs,\n","    callbacks=[early_stopping],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1716125645053,"user":{"displayName":"Dimitris Z","userId":"18123542160357697530"},"user_tz":-120},"id":"ecbZmc8dtvBL","outputId":"1457cefc-afa8-4f6a-837f-271b65a22b42"},"outputs":[],"source":["# Plot training & validation loss values\n","plt.plot(history.history['loss'], label='Train Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Model Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend(loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"801RmJB2rI5E"},"source":["# Create inference model and test inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3679,"status":"ok","timestamp":1716125648714,"user":{"displayName":"Dimitris Z","userId":"18123542160357697530"},"user_tz":-120},"id":"CnB8qI6Hv_M8","outputId":"817344df-2a76-4bb9-b96c-ff790afd4468"},"outputs":[],"source":["def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1):\n","    input_shape = tf.shape(y_pred)\n","    num_samples, num_steps = input_shape[0], input_shape[1]\n","    y_pred = tf.math.log(tf.transpose(y_pred, perm=[1, 0, 2]) + tf.keras.backend.epsilon())\n","    input_length = tf.cast(input_length, dtype=tf.int32)\n","\n","    if greedy:\n","        (decoded, log_prob) = tf.nn.ctc_greedy_decoder(\n","            inputs=y_pred, sequence_length=input_length\n","        )\n","    else:\n","        (decoded, log_prob) = tf.nn.ctc_beam_search_decoder(\n","            inputs=y_pred,\n","            sequence_length=input_length,\n","            beam_width=beam_width,\n","            top_paths=top_paths,\n","        )\n","    decoded_dense = []\n","    for st in decoded:\n","        st = tf.SparseTensor(st.indices, st.values, (num_samples, num_steps))\n","        decoded_dense.append(tf.sparse.to_dense(sp_input=st, default_value=-1))\n","    return (decoded_dense, log_prob)\n","\n","prediction_model = tf.keras.Model(\n","    inputs=model.input[0],\n","    outputs=model.get_layer(name=\"dense2\").output,\n","    name=\"ocr_inference\"\n",")\n","prediction_model.summary()\n","\n","\n","def decode_batch_predictions(pred, max_length, num_to_char):\n","    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n","    # Use greedy search. For complex tasks, you can use beam search\n","    results = ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :max_length]\n","    # Iterate over the results and get back the text\n","    output_text = []\n","    for res in results:\n","        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n","        output_text.append(res)\n","    return output_text\n","\n","# Assuming val_dataset, num_to_char, and max_length are defined\n","for batch in val_dataset.take(1):\n","    batch_images = batch[\"image\"]\n","    batch_labels = batch[\"label\"]\n","\n","    preds = prediction_model.predict(batch_images)\n","    pred_texts = decode_batch_predictions(preds, MAX_SEQ_LENGTH, num_to_char)\n","\n","    orig_texts = []\n","    for label in batch_labels:\n","        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n","        orig_texts.append(label)\n","\n","    num_predictions = len(pred_texts)\n","    num_rows = min(4, num_predictions)\n","    num_cols = min(4, num_predictions)\n","\n","    _, ax = plt.subplots(num_rows, num_cols, figsize=(15, 5))\n","    for i in range(16):\n","        img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n","        img = img.T\n","        title = f\"Prediction: {pred_texts[i]}\"\n","        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n","        ax[i // 4, i % 4].set_title(title, fontsize = 8)\n","        ax[i // 4, i % 4].axis(\"off\")\n","\n","    plt.suptitle(\"Predictions\", fontsize=16)\n","    plt.subplots_adjust(hspace=0.5)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OFjhsWTr4L0I"},"source":["# Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13309,"status":"ok","timestamp":1716125662018,"user":{"displayName":"Dimitris Z","userId":"18123542160357697530"},"user_tz":-120},"id":"fcWSOs6j3lhd","outputId":"f07bd204-c1dd-4faa-8298-725730c6b611"},"outputs":[],"source":["all_pred_chars = []\n","all_true_chars = []\n","\n","test_iterator = iter(test_dataset)\n","while True:\n","    try:\n","        batch = next(test_iterator)\n","        batch_images = batch[\"image\"]\n","        batch_labels = batch[\"label\"]\n","\n","        # Predict labels using the trained model\n","        preds = prediction_model.predict(batch_images, verbose=0)\n","        pred_texts = decode_batch_predictions(preds, MAX_SEQ_LENGTH, num_to_char)\n","\n","        # Convert ground truth labels to human-readable text\n","        true_texts = []\n","        for label in batch_labels:\n","            true_texts.append(tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\"))\n","\n","        # Flatten the predicted and true texts to get individual characters\n","        pred_chars = [char for text in pred_texts for char in text]\n","        true_chars = [char for text in true_texts for char in text]\n","\n","        # Ensure consistent sequence lengths\n","        max_len = MAX_SEQ_LENGTH\n","        pred_chars = pred_chars[:max_len] + [-1] * (max_len - len(pred_chars))\n","        true_chars = true_chars[:max_len]\n","\n","        # Accumulate predictions and ground truth labels for individual characters\n","        all_pred_chars.extend(pred_chars)\n","        all_true_chars.extend(true_chars)\n","\n","    except StopIteration:\n","        break\n","\n","# Calculate character accuracy\n","char_accuracy = accuracy_score(all_true_chars, all_pred_chars)\n","\n","# Calculate precision, recall, and F1-score for each class (character)\n","precision, recall, f1, _ = precision_recall_fscore_support(all_true_chars, all_pred_chars, average=None)\n","\n","# Calculate average precision, recall, and F1-score\n","avg_precision, avg_recall, avg_f1, _ = precision_recall_fscore_support(all_true_chars, all_pred_chars, average='weighted')\n","\n","# Print performance metrics\n","print(\"Character Accuracy:\", char_accuracy)\n","print(f\"\\nAverage Precision: {avg_precision} | Average Recall: {avg_recall} | Average F1: {avg_f1}\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(all_true_chars, all_pred_chars, zero_division= 0))"]},{"cell_type":"markdown","metadata":{"id":"8AgJq9l48ICN"},"source":["# Save model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21315,"status":"ok","timestamp":1716125683310,"user":{"displayName":"Dimitris Z","userId":"18123542160357697530"},"user_tz":-120},"id":"RAngQgSJ8IuE","outputId":"63ab87e3-62d6-4058-f976-80ed54ab49e6"},"outputs":[],"source":["import os\n","\n","# Function to save the trained model\n","def save_model(model_to_save, model_dir):\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","    model_to_save.save(model_dir)\n","\n","# Save the model\n","save_model(prediction_model, 'ocr_artifacts/ocr_model/')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
